{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from dd_class import Dragodinde\n",
    "from dd_class import Elevage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElevageEnv(gym.Env):\n",
    "    def __init__(self, elevage):\n",
    "        super(ElevageEnv, self).__init__()\n",
    "        self.elevage = elevage\n",
    "\n",
    "        # Define the action and observation space\n",
    "        self.action_space = spaces.Discrete(len(elevage.dragodindes) ** 2)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=10, shape=(len(elevage.dragodindes), 4), dtype=np.float32)\n",
    "\n",
    "        self.state = self._get_observation()\n",
    "        self.current_step = 0\n",
    "        self.max_steps = 1000\n",
    "        self.max_generations = 10\n",
    "        self.generation = 1\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Returns the current observation of the environment.\n",
    "        The observation could be the current state of all dragodindes in the elevage.\n",
    "        Each dragodinde might have features such as gender, color, generation, etc.\n",
    "        \"\"\"\n",
    "        obs = []\n",
    "        for dragodinde in self.elevage.dragodindes:\n",
    "            obs.append([dragodinde.id, dragodinde.generation, self._encode_gender(dragodinde.gender), self._encode_color(dragodinde.color)])\n",
    "        return np.array(obs, dtype=np.float32)\n",
    "\n",
    "    def _encode_gender(self, gender):\n",
    "        \"\"\"Encodes gender as a numerical value.\"\"\"\n",
    "        return 1 if gender == \"M\" else 0\n",
    "\n",
    "    def _encode_color(self, color):\n",
    "        \"\"\"Encodes color as a numerical value. Extend this method if more colors are introduced.\"\"\"\n",
    "        color_encoding = {\"Rousse\": 0, \"Amande\": 1, \"Dorée\": 2}\n",
    "        return color_encoding.get(color, -1)  # Default to -1 if color not found\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Apply the action and return the next state, reward, done, and info.\n",
    "        \"\"\"\n",
    "        assert self.action_space.contains(action)\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Define the logic for updating the state based on the action\n",
    "        # For now, let's assume action is an index to select two dragodindes for breeding\n",
    "        dragodinde_1_idx = action // len(self.elevage.dragodindes)\n",
    "        dragodinde_2_idx = action % len(self.elevage.dragodindes)\n",
    "\n",
    "        self._breed_dragodindes(dragodinde_1_idx, dragodinde_2_idx)\n",
    "\n",
    "        # Calculate the reward based on the action\n",
    "        next_state, reward, done, info = self._calculate_reward(action)\n",
    "\n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        self.state = self._get_observation()\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def _breed_dragodindes(self, idx1, idx2):\n",
    "        \"\"\"\n",
    "        Simulates the breeding of two dragodindes and updates the elevage state.\n",
    "        This is a placeholder method and should be replaced with actual breeding logic.\n",
    "        \"\"\"\n",
    "        if idx1 != idx2:  # Ensure that the same dragodinde is not breeding with itself\n",
    "            parent_1 = self.elevage.dragodindes[idx1]\n",
    "            parent_2 = self.elevage.dragodindes[idx2]\n",
    "            # Implement actual breeding logic here and update self.elevage.dragodindes\n",
    "            # For simplicity, assume a new dragodinde is created with generation + 1\n",
    "            new_dragodinde = Dragodinde(\n",
    "                id=len(self.elevage.dragodindes) + 1,\n",
    "                gender=\"M\" if np.random.rand() > 0.5 else \"F\",\n",
    "                color=parent_1.color,  # Assume the new dragodinde inherits color from parent_1\n",
    "                generation=max(parent_1.generation, parent_2.generation) + 1\n",
    "            )\n",
    "            self.elevage.dragodindes.append(new_dragodinde)\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        \"\"\"\n",
    "        Calculates the reward based on the action and the current state of the environment.\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        done = False\n",
    "        info = {}\n",
    "\n",
    "        if self.generation >= self.max_generations:\n",
    "            done = True\n",
    "            reward = 1000  # High reward for completing the maximum generations\n",
    "\n",
    "        elif action == self.generation:\n",
    "            reward = 100  # Positive reward for advancing the generation\n",
    "\n",
    "        elif action > self.generation:\n",
    "            reward = 50  # Smaller reward for valid actions advancing the generation\n",
    "\n",
    "        elif action < self.generation - 2:\n",
    "            reward = -100  # Penalty for regressing too far back in generations\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to an initial state and returns the initial observation.\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "        self.generation = 1\n",
    "        self.elevage = self.create_elevage()\n",
    "        self.state = self._get_observation()\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Renders the current state of the environment.\n",
    "        \"\"\"\n",
    "        print(f\"Generation: {self.generation}, State: {self.state}\")\n",
    "\n",
    "    def create_elevage(self):\n",
    "        \"\"\"\n",
    "        Initializes a new Elevage with a predefined set of dragodindes.\n",
    "        \"\"\"\n",
    "        dragodindes_data = [\n",
    "            (1, \"M\", \"Rousse\", 1),\n",
    "            (2, \"F\", \"Rousse\", 1),\n",
    "            (3, \"M\", \"Amande\", 1),\n",
    "            (4, \"F\", \"Amande\", 1),\n",
    "            (5, \"M\", \"Dorée\", 1),\n",
    "            (6, \"F\", \"Dorée\", 1)\n",
    "        ]\n",
    "\n",
    "        list_dd = []\n",
    "        for id, gender, color, generation in dragodindes_data:\n",
    "            dragodinde = Dragodinde(id, gender, color, generation)\n",
    "            list_dd.append(dragodinde)\n",
    "\n",
    "        return Elevage(list_dd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(layers.Dense(64, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(layers.Dense(32, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(layers.Dense(16, activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size, activation='linear'))\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
